{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-05T14:13:56.152395Z","iopub.execute_input":"2021-10-05T14:13:56.152795Z","iopub.status.idle":"2021-10-05T14:13:56.163426Z","shell.execute_reply.started":"2021-10-05T14:13:56.152759Z","shell.execute_reply":"2021-10-05T14:13:56.161943Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"# Per prima cosa importiamo la libreria trasformers e il dataset di interesse\n\n!pip install transformers\n\ndataenj = pd.read_csv('../input/dataset-di-poesie-italiane-800900/Dataset enjambement.txt')\ndataenj","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:13:56.166398Z","iopub.execute_input":"2021-10-05T14:13:56.167553Z","iopub.status.idle":"2021-10-05T14:14:02.440831Z","shell.execute_reply.started":"2021-10-05T14:13:56.167458Z","shell.execute_reply":"2021-10-05T14:14:02.439856Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"#essendo il dataset originale ordinato per label, ci assicuriamo di randomizzarlo prima di utilizzarlo\n\ndfenj = dataenj.sample(frac=1).reset_index(drop=True)\ndfenj","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:02.444203Z","iopub.execute_input":"2021-10-05T14:14:02.444475Z","iopub.status.idle":"2021-10-05T14:14:02.460166Z","shell.execute_reply.started":"2021-10-05T14:14:02.444446Z","shell.execute_reply":"2021-10-05T14:14:02.458985Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"#Suddividiamo il dataset in train set e test set dopo aver randomizzato l'ordine dei record.\n\nimport math\nn_train = math.floor(400*0.8)\n\ntrain_df = dfenj[:n_train]\ntest_df = dfenj[n_train:]\n\n\n# Mostriamo il numero di frasi per le due suddivisioni\nprint('Numero di frasi di training: {:,}\\n'.format(train_df.shape[0]))\nprint('Numero di frasi di testing: {:,}\\n'.format(test_df.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:02.463253Z","iopub.execute_input":"2021-10-05T14:14:02.463720Z","iopub.status.idle":"2021-10-05T14:14:02.471859Z","shell.execute_reply.started":"2021-10-05T14:14:02.463689Z","shell.execute_reply":"2021-10-05T14:14:02.470678Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"# Visualizziamo 10 frasi random dal dataset di training creato.\n\ntrain_df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:02.474934Z","iopub.execute_input":"2021-10-05T14:14:02.475782Z","iopub.status.idle":"2021-10-05T14:14:02.489759Z","shell.execute_reply.started":"2021-10-05T14:14:02.475743Z","shell.execute_reply":"2021-10-05T14:14:02.488805Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"Per poter essere utilizzate come input nel modello BERT, le frasi e le corrispondenti etichette devono essere prima **preprocessate**.\nPrima si estraggono i valori (testo e label) separatamente, dopo si procede con la **tokenizzazione** del testo e la **mappatura dei tokens** sul corrispettivo indice del vocabolario.","metadata":{}},{"cell_type":"code","source":"train_sentences = train_df.testo.values\ntrain_labels = train_df.label_enj.values\n\ntest_sentences = test_df.testo.values\ntest_labels = test_df.label_enj.values","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:02.491685Z","iopub.execute_input":"2021-10-05T14:14:02.492172Z","iopub.status.idle":"2021-10-05T14:14:02.499013Z","shell.execute_reply.started":"2021-10-05T14:14:02.492133Z","shell.execute_reply":"2021-10-05T14:14:02.498156Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"# Utilizziamo un tokenizer preallenato sulla lingua italiana\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-italian-xxl-cased\")","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:02.500696Z","iopub.execute_input":"2021-10-05T14:14:02.501284Z","iopub.status.idle":"2021-10-05T14:14:03.303708Z","shell.execute_reply.started":"2021-10-05T14:14:02.501247Z","shell.execute_reply":"2021-10-05T14:14:03.302644Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"print(' Originale: ', train_sentences[0])\n\nprint('Tokenizzata: ', tokenizer.tokenize(train_sentences[0]))\n\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_sentences[0])))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:03.305914Z","iopub.execute_input":"2021-10-05T14:14:03.306561Z","iopub.status.idle":"2021-10-05T14:14:03.315743Z","shell.execute_reply.started":"2021-10-05T14:14:03.306508Z","shell.execute_reply":"2021-10-05T14:14:03.314519Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"Per formattare correttamente l'input del modello BERT sarà necessario:\n\n1. aggiungere dei token speciali all'inizio e alla fine di ogni frase (**'[CLS]'** e **'[SEP]'**),\n2. aggiungere **padding** e **troncare** le frasi in modo che i tensori corrispondenti abbiano tutte la stessa lunghezza,\n3. differenziare i token \"pieni\" dai token di padding grazie al tensore di **attention mask**.\n\nPer stabilire la lunghezza finale ideale delle frasi si è calcolata prima la lunghezza e poi la distribuzione delle lunghezze all'interno del corpus.","metadata":{}},{"cell_type":"code","source":"# Calcoliamo la stringa di testo più lungo nel dataset di training\n\nmax_len_train = 0\nfor sent in train_sentences:\n    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n    max_len_train = max(max_len_train, len(input_ids))\n\nprint('Lunghezza massima: ', max_len_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:03.317388Z","iopub.execute_input":"2021-10-05T14:14:03.317881Z","iopub.status.idle":"2021-10-05T14:14:03.460173Z","shell.execute_reply.started":"2021-10-05T14:14:03.317838Z","shell.execute_reply":"2021-10-05T14:14:03.459228Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"# Visualizziamo la distribuzione delle lunghezze \n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\ntokens_len = []\n\nfor sent in train_sentences:\n    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n    tokens_len.append(len(input_ids))\n    \nsns.distplot(tokens_len)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:03.461493Z","iopub.execute_input":"2021-10-05T14:14:03.461999Z","iopub.status.idle":"2021-10-05T14:14:03.754450Z","shell.execute_reply.started":"2021-10-05T14:14:03.461958Z","shell.execute_reply":"2021-10-05T14:14:03.753501Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"# Visualizzando i dati in boxplot si evidenziano gli outliers\n\nsns.boxplot(tokens_len)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:03.757602Z","iopub.execute_input":"2021-10-05T14:14:03.757879Z","iopub.status.idle":"2021-10-05T14:14:03.862964Z","shell.execute_reply.started":"2021-10-05T14:14:03.757850Z","shell.execute_reply":"2021-10-05T14:14:03.862014Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"markdown","source":"Procediamo con la **tokenizzazione**, **definizione degli ID** e delle **attention mask** per tutte le frasi del dataset di training. \nDato che la lunghezza massima è comunque piccola rispetto a quella supportata, si è scelto di usare un valore più alto per assicurarci che non avvengano troncamenti di alcun tipo.","metadata":{}},{"cell_type":"code","source":"input_ids = []\nattention_masks = []\n\nfor sent in train_sentences:\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      \n                        add_special_tokens = True, # Aggiunta di '[CLS]' e '[SEP]'\n                        max_length = 80,          # Padding\n                        padding= 'max_length',\n                        truncation=True,\n                        return_attention_mask = True,   # Costruzione delle att. mask.\n                        return_tensors = 'pt',     # Restituzione dei tensori pytorch\n                   )\n        \n    input_ids.append(encoded_dict['input_ids'])\n    \n    attention_masks.append(encoded_dict['attention_mask'])\n\n# Trasformiamo le liste in tensori\ntrain_input_ids = torch.cat(input_ids, dim=0)\ntrain_attention_masks = torch.cat(attention_masks, dim=0)\ntrain_labels = torch.tensor(train_labels)\n\n# Visualizziamo il risultato per una frase\nprint('Originale: ', train_sentences[0])\nprint('Token IDs:', train_input_ids[0])\nprint('Attention mask', train_attention_masks[0])","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:03.864686Z","iopub.execute_input":"2021-10-05T14:14:03.865062Z","iopub.status.idle":"2021-10-05T14:14:04.096893Z","shell.execute_reply.started":"2021-10-05T14:14:03.865015Z","shell.execute_reply":"2021-10-05T14:14:04.095932Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"Seguiamo lo stesso procedimento per il dataset di test.","metadata":{}},{"cell_type":"code","source":"max_len = 0\n\nfor sent in test_sentences:\n    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n    max_len = max(max_len, len(input_ids))\n    \ntokens_len = []\n\nfor sent in test_sentences:\n    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n    tokens_len.append(len(input_ids))\n\nprint('Lunghezza massima: ', max_len)\nsns.distplot(tokens_len)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:04.098673Z","iopub.execute_input":"2021-10-05T14:14:04.099951Z","iopub.status.idle":"2021-10-05T14:14:04.466343Z","shell.execute_reply.started":"2021-10-05T14:14:04.099910Z","shell.execute_reply":"2021-10-05T14:14:04.465240Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(tokens_len)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:04.470853Z","iopub.execute_input":"2021-10-05T14:14:04.471291Z","iopub.status.idle":"2021-10-05T14:14:04.634796Z","shell.execute_reply.started":"2021-10-05T14:14:04.471249Z","shell.execute_reply":"2021-10-05T14:14:04.633857Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"markdown","source":"Il valore scelto come lunghezza massima per il test è di **80**, come per le frasi di train","metadata":{}},{"cell_type":"code","source":"input_ids = []\nattention_masks = []\n\n\nfor sent in test_sentences:\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      \n                        add_special_tokens = True,\n                        max_length = 80,           \n                        padding='max_length',\n                        truncation=True,\n                        return_attention_mask = True,   \n                        return_tensors = 'pt',     \n                   )\n      \n    input_ids.append(encoded_dict['input_ids'])\n    \n    attention_masks.append(encoded_dict['attention_mask'])\n\ntest_input_ids = torch.cat(input_ids, dim=0)\ntest_attention_masks = torch.cat(attention_masks, dim=0)\ntest_labels = torch.tensor(test_labels)\n\nprint('Original: ', test_sentences[50])\nprint('Token IDs:', test_input_ids[50])\nprint('Attention mask', test_attention_masks[50])","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:04.636007Z","iopub.execute_input":"2021-10-05T14:14:04.636420Z","iopub.status.idle":"2021-10-05T14:14:04.702455Z","shell.execute_reply.started":"2021-10-05T14:14:04.636375Z","shell.execute_reply":"2021-10-05T14:14:04.700785Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":"Il dataset viene diviso randomicamente in 90% training set e 10% validation set","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, random_split\n\n# Si combinano gli input di training in un TensorDataset.\ntrain_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\ntest_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n\n# Calcoliamo il numero di samples da includere in ciascun set.\ntrain_size = int(0.9 * len(train_dataset))\nval_size = len(train_dataset) - train_size\n\n# Dividiamo il dataset randomicamente.\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:04.704813Z","iopub.execute_input":"2021-10-05T14:14:04.705288Z","iopub.status.idle":"2021-10-05T14:14:04.716182Z","shell.execute_reply.started":"2021-10-05T14:14:04.705238Z","shell.execute_reply":"2021-10-05T14:14:04.714195Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":"Dato che il task di nostro interesse consiste nella classificazione di frasi, importiamo la classe **BertForSequenceClassification** per il fine-tuning del modello BERT.","metadata":{}},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, AdamW, BertConfig\n \nmodel = BertForSequenceClassification.from_pretrained(\n    \"dbmdz/bert-base-italian-xxl-cased\", # lo stesso usato per il tokenizzatore\n    num_labels = 2,  # il numero di classi che ci interessa\n    output_attentions = False, \n    output_hidden_states = False,\n)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:04.718190Z","iopub.execute_input":"2021-10-05T14:14:04.718793Z","iopub.status.idle":"2021-10-05T14:14:08.606710Z","shell.execute_reply.started":"2021-10-05T14:14:04.718709Z","shell.execute_reply":"2021-10-05T14:14:08.605810Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"markdown","source":"Eseguiamo il training del modello per 4 epoche, utilizzando l'ottimizzatore **AdamW** e un **batch-size pari a 16**. Alla fine dell'allenamento è possibile visualizzare un grafico dell'andamento della **loss** e dell'**accuracy** nelle 4 epoche.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.nn import functional as F\n\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\n\nmodel.to(device)\nmodel.train()\n\noptim = AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n\nloaders = {\"train\" : train_loader,\n           \"val\" : val_loader,\n           \"test\" : test_loader}\n\nhistory_loss = {\"train\":[], \"val\":[], \"test\":[]}\nhistory_accuracy = {\"train\":[], \"val\":[], \"test\":[]}\n\ntry:\n  for epoch in range(4):\n    sum_loss = {\"train\":0, \"val\":0, \"test\":0}\n    sum_accuracy = {\"train\":0, \"val\":0, \"test\":0}\n    for split in [\"train\",\"val\",\"test\"]:\n      print(f\"Processing {split}\")\n      for batch in loaders[split]:\n          optim.zero_grad()\n          # `batch` contains three pytorch tensors:\n          #  [0]: input ids \n          #  [1]: attention masks\n          #  [2]: labels\n          input_ids = batch[0].to(device)\n          attn_mask = batch[1].to(device)\n          labels = batch[2].to(device)\n          output = model(input_ids, token_type_ids=None, attention_mask=attn_mask, labels=labels, return_dict=True)\n          loss = output[0]\n          #logits -> batch_size x num_labels\n          logits = output[1]\n          sum_loss[split] += loss.item()\n          if split == \"train\":\n            loss.backward()\n            optim.step()\n          pred = torch.argmax(logits, dim = 1)\n          batch_accuracy = (pred == labels).sum().item()/(input_ids.size(0))\n          sum_accuracy[split] += batch_accuracy\n    epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\",\"val\",\"test\"]}\n    epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\",\"val\",\"test\"]}\n    for split in[\"train\",\"val\",\"test\"]:\n      history_loss[split].append(epoch_loss[split])\n      history_accuracy[split].append(epoch_accuracy[split])\n    print(f\"Epoch {epoch+1}:\",\n          f\"TrL={epoch_loss['train']:.4f},\",\n          f\"TrA={epoch_accuracy['train']:.4f},\",\n          f\"VL={epoch_loss['val']:.4f},\",\n          f\"VA={epoch_accuracy['val']:.4f},\",\n          f\"TeL={epoch_loss['test']:.4f},\",\n          f\"TeA={epoch_accuracy['test']:.4f},\")\nexcept KeyboardInterrupt:\n  print(\"interrupted\")\nfinally:\n  plt.title(\"loss\")\n  for split in [\"train\",\"val\",\"test\"]:\n    plt.plot(history_loss[split], label=split)\n  plt.legend()\n  plt.show()\n\n  plt.title(\"accuracy\")\n  for split in [\"train\",\"val\",\"test\"]:\n    plt.plot(history_accuracy[split], label=split)\n  plt.legend()\n  plt.show()\n    \n#torch.save(model.state_dict(), \"./modello4.4\")","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:08.608196Z","iopub.execute_input":"2021-10-05T14:14:08.608571Z","iopub.status.idle":"2021-10-05T14:14:21.490482Z","shell.execute_reply.started":"2021-10-05T14:14:08.608530Z","shell.execute_reply":"2021-10-05T14:14:21.489631Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:21.494491Z","iopub.execute_input":"2021-10-05T14:14:21.494775Z","iopub.status.idle":"2021-10-05T14:14:21.505337Z","shell.execute_reply.started":"2021-10-05T14:14:21.494745Z","shell.execute_reply":"2021-10-05T14:14:21.504155Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"markdown","source":"Per avere ulteriori metriche di riferimento per la valutazione del modello, abbiamo deciso di plottare la **matrice di confusione** e di calcolare il **classification report** sui risultati della parte di test.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\nx_test = test_df.testo.values\ny_test = test_df.label_enj.values\n\ny_pred=[]\nfor x in x_test:\n    input_encodings = tokenizer(x, return_tensors='pt')\n    input_ids = input_encodings[\"input_ids\"].to(device)\n    attn_mask = input_encodings[\"attention_mask\"].to(device)\n    output = model(input_ids, attention_mask=attn_mask)\n    pred = torch.argmax(output[0], dim = 1)\n    y_pred.append(pred.item())    \n    \nprint(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:21.507053Z","iopub.execute_input":"2021-10-05T14:14:21.507519Z","iopub.status.idle":"2021-10-05T14:14:22.608166Z","shell.execute_reply.started":"2021-10-05T14:14:21.507435Z","shell.execute_reply":"2021-10-05T14:14:22.607219Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"cm= confusion_matrix(y_test,y_pred)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot() ","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:22.609461Z","iopub.execute_input":"2021-10-05T14:14:22.609960Z","iopub.status.idle":"2021-10-05T14:14:22.760621Z","shell.execute_reply.started":"2021-10-05T14:14:22.609921Z","shell.execute_reply":"2021-10-05T14:14:22.759743Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"markdown","source":"Possiamo testare manualmente il nostro modello con frasi di prova.","metadata":{}},{"cell_type":"code","source":"frase = \"il vento\\r\\nfresco soffiava\"\n\ninput_encodings = tokenizer(frase, return_tensors='pt')\ninput_ids = input_encodings[\"input_ids\"].to(device)\nattn_mask = input_encodings[\"attention_mask\"].to(device)\noutput = model(input_ids, attention_mask=attn_mask)\npred = torch.argmax(output[0], dim = 1)\nprint(pred.item())","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:22.763599Z","iopub.execute_input":"2021-10-05T14:14:22.763876Z","iopub.status.idle":"2021-10-05T14:14:22.789532Z","shell.execute_reply.started":"2021-10-05T14:14:22.763850Z","shell.execute_reply":"2021-10-05T14:14:22.788677Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"frase = \"ero stanca.\\r\\nSapevo che\"\n\ninput_encodings = tokenizer(frase, return_tensors='pt')\ninput_ids = input_encodings[\"input_ids\"].to(device)\nattn_mask = input_encodings[\"attention_mask\"].to(device)\noutput = model(input_ids, attention_mask=attn_mask)\npred = torch.argmax(output[0], dim = 1)\nprint(pred.item())","metadata":{"execution":{"iopub.status.busy":"2021-10-05T14:14:22.791118Z","iopub.execute_input":"2021-10-05T14:14:22.791501Z","iopub.status.idle":"2021-10-05T14:14:22.818290Z","shell.execute_reply.started":"2021-10-05T14:14:22.791462Z","shell.execute_reply":"2021-10-05T14:14:22.817381Z"},"trusted":true},"execution_count":138,"outputs":[]}]}