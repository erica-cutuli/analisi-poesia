{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-05T17:37:21.050356Z","iopub.execute_input":"2021-10-05T17:37:21.050853Z","iopub.status.idle":"2021-10-05T17:37:21.062309Z","shell.execute_reply.started":"2021-10-05T17:37:21.050808Z","shell.execute_reply":"2021-10-05T17:37:21.061192Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"# Per prima cosa importiamo la libreria trasformers e il dataset di interesse\n\n!pip install transformers\n\ndatasim = pd.read_csv('../input/dataset-di-poesie-italiane-800900/Dataset similitudini.txt')\ndatasim","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:21.064507Z","iopub.execute_input":"2021-10-05T17:37:21.064945Z","iopub.status.idle":"2021-10-05T17:37:27.351563Z","shell.execute_reply.started":"2021-10-05T17:37:21.064908Z","shell.execute_reply":"2021-10-05T17:37:27.350668Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"#essendo il dataset originale ordinato per label, ci assicuriamo di randomizzarlo prima di utilizzarlo\n\ndfsim = datasim.sample(frac=1).reset_index(drop=True)\ndfsim","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:27.353222Z","iopub.execute_input":"2021-10-05T17:37:27.353575Z","iopub.status.idle":"2021-10-05T17:37:27.371691Z","shell.execute_reply.started":"2021-10-05T17:37:27.353535Z","shell.execute_reply":"2021-10-05T17:37:27.370567Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"#Suddividiamo il dataset in train set e test set dopo aver randomizzato l'ordine dei record.\n\nimport math\nn_train = math.floor(180*0.8)\n\ntrain_df = dfsim[:n_train]\ntest_df = dfsim[n_train:]\n\n\n# Mostriamo il numero di frasi per le due suddivisioni\nprint('Numero di frasi di training: {:,}\\n'.format(train_df.shape[0]))\nprint('Numero di frasi di testing: {:,}\\n'.format(test_df.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:27.373544Z","iopub.execute_input":"2021-10-05T17:37:27.373988Z","iopub.status.idle":"2021-10-05T17:37:27.381373Z","shell.execute_reply.started":"2021-10-05T17:37:27.373951Z","shell.execute_reply":"2021-10-05T17:37:27.380555Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"# Visualizziamo 10 frasi random dal dataset di training creato.\n\ntrain_df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:27.384850Z","iopub.execute_input":"2021-10-05T17:37:27.385361Z","iopub.status.idle":"2021-10-05T17:37:27.402930Z","shell.execute_reply.started":"2021-10-05T17:37:27.385322Z","shell.execute_reply":"2021-10-05T17:37:27.401841Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"Per poter essere utilizzate come input nel modello BERT, le frasi e le corrispondenti etichette devono essere prima **preprocessate**.\nPrima si estraggono i valori (testo e label) separatamente, dopo si procede con la **tokenizzazione** del testo e la **mappatura dei tokens** sul corrispettivo indice del vocabolario.","metadata":{}},{"cell_type":"code","source":"#essendo l'input salvato in due colonne, si devono prima concatenare i due campi\n\ntrain_labels = train_df.label_sim.values\ntrs1 = [i if type(i)==str else \"\" for i in list(train_df.termine_uno.values)]\ntrs2 = list(train_df.termine_due.values)\ntrain_sentences = []\nfor a,b in zip(trs1,trs2):\n    train_sentences.append(a+\" \"+b)\ntrain_sentences = np.array(train_sentences)\n\ntest_labels = test_df.label_sim.values\ntes1 = [i if type(i)==str else \"\" for i in list(test_df.termine_uno.values)]\ntes2 = list(test_df.termine_due.values)\ntest_sentences = []\nfor a,b in zip(tes1,tes2):\n    test_sentences.append(a+\" \"+b)\ntest_sentences = np.array(test_sentences)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:27.405990Z","iopub.execute_input":"2021-10-05T17:37:27.406481Z","iopub.status.idle":"2021-10-05T17:37:27.418183Z","shell.execute_reply.started":"2021-10-05T17:37:27.406440Z","shell.execute_reply":"2021-10-05T17:37:27.417123Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"# Utilizziamo un tokenizer preallenato sulla lingua italiana\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-italian-xxl-cased\")","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:27.419701Z","iopub.execute_input":"2021-10-05T17:37:27.420321Z","iopub.status.idle":"2021-10-05T17:37:29.601629Z","shell.execute_reply.started":"2021-10-05T17:37:27.420282Z","shell.execute_reply":"2021-10-05T17:37:29.600717Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"# Visualizziamo il risultato del tokenizzatore su un'altra frase\n\nprint(' Originale: ', train_sentences[10])\n\nprint('Tokenizzata: ', tokenizer.tokenize(train_sentences[10]))\n\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_sentences[10])))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:29.606229Z","iopub.execute_input":"2021-10-05T17:37:29.608408Z","iopub.status.idle":"2021-10-05T17:37:29.622392Z","shell.execute_reply.started":"2021-10-05T17:37:29.608364Z","shell.execute_reply":"2021-10-05T17:37:29.621469Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"Per formattare correttamente l'input del modello BERT dovremo:\n\n1. aggiungere dei token speciali all'inizio e alla fine di ogni frase (**'[CLS]'** e **'[SEP]'**),\n2. aggiungere **padding** e **troncare** le frasi in modo che i tensori corrispondenti abbiano tutti la stessa lunghezza,\n3. differenziare i token \"pieni\" dai token di padding attraverso il tensore di **attention mask**.\n\nPer stabilire la lunghezza finale ideale delle frasi si è calcolata prima la lunghezza e poi la distribuzione delle lunghezze all'interno del corpus.","metadata":{}},{"cell_type":"code","source":"# Calcoliamo la stringa di testo più lungo nel dataset di training\n\nmax_len_train = 0\nfor sent in train_sentences:\n    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n    max_len_train = max(max_len_train, len(input_ids))\n\nprint('Lunghezza massima: ', max_len_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:29.626460Z","iopub.execute_input":"2021-10-05T17:37:29.628583Z","iopub.status.idle":"2021-10-05T17:37:29.720216Z","shell.execute_reply.started":"2021-10-05T17:37:29.628542Z","shell.execute_reply":"2021-10-05T17:37:29.719331Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"# Visualizziamo la distribuzione delle lunghezze \n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\ntokens_len = []\n\nfor sent in train_sentences:\n    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n    tokens_len.append(len(input_ids))\n    \nsns.distplot(tokens_len)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:29.724265Z","iopub.execute_input":"2021-10-05T17:37:29.726548Z","iopub.status.idle":"2021-10-05T17:37:30.012896Z","shell.execute_reply.started":"2021-10-05T17:37:29.726505Z","shell.execute_reply":"2021-10-05T17:37:30.012117Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"# Visualizzando i dati in boxplot si evidenziano gli outliers\n\nsns.boxplot(tokens_len)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:30.014491Z","iopub.execute_input":"2021-10-05T17:37:30.014892Z","iopub.status.idle":"2021-10-05T17:37:30.129064Z","shell.execute_reply.started":"2021-10-05T17:37:30.014862Z","shell.execute_reply":"2021-10-05T17:37:30.127854Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"markdown","source":"Procediamo con la **tokenizzazione**, **definizione degli ID** e delle **attention mask** per tutte le frasi del dataset di training. \nDato che la lunghezza massima è comunque piccola rispetto a quella supportata, si è scelto di usare un valore più alto per assicurarci che non avvengano troncamenti di alcun tipo.","metadata":{}},{"cell_type":"code","source":"input_ids = []\nattention_masks = []\n\nfor sent in train_sentences:\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      \n                        add_special_tokens = True, # Aggiunta di '[CLS]' e '[SEP]'\n                        max_length = 80,          # Padding\n                        padding= 'max_length',\n                        truncation=True,\n                        return_attention_mask = True,   # Costruzione delle att. mask.\n                        return_tensors = 'pt',     # Restituzione dei tensori pytorch\n                   )\n        \n    input_ids.append(encoded_dict['input_ids'])\n    \n    attention_masks.append(encoded_dict['attention_mask'])\n\n# Trasformiamo le liste in tensori\ntrain_input_ids = torch.cat(input_ids, dim=0)\ntrain_attention_masks = torch.cat(attention_masks, dim=0)\ntrain_labels = torch.tensor(train_labels)\n\n# Visualizziamo il risultato per una frase\nprint('Originale: ', train_sentences[0])\nprint('Token IDs:', train_input_ids[0])\nprint('Attention mask', train_attention_masks[0])","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:30.131436Z","iopub.execute_input":"2021-10-05T17:37:30.131940Z","iopub.status.idle":"2021-10-05T17:37:30.216606Z","shell.execute_reply.started":"2021-10-05T17:37:30.131886Z","shell.execute_reply":"2021-10-05T17:37:30.215670Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"Seguiamo lo stesso procedimento per il dataset di test.","metadata":{}},{"cell_type":"code","source":"max_len = 0\n\nfor sent in test_sentences:\n    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n    max_len = max(max_len, len(input_ids))\n    \ntokens_len = []\n\nfor sent in test_sentences:\n    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n    tokens_len.append(len(input_ids))\n\nprint('Lunghezza massima: ', max_len)\nsns.distplot(tokens_len)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:30.218487Z","iopub.execute_input":"2021-10-05T17:37:30.219107Z","iopub.status.idle":"2021-10-05T17:37:30.381718Z","shell.execute_reply.started":"2021-10-05T17:37:30.219069Z","shell.execute_reply":"2021-10-05T17:37:30.380705Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(tokens_len)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:30.383201Z","iopub.execute_input":"2021-10-05T17:37:30.383636Z","iopub.status.idle":"2021-10-05T17:37:30.489887Z","shell.execute_reply.started":"2021-10-05T17:37:30.383589Z","shell.execute_reply":"2021-10-05T17:37:30.489205Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"input_ids = []\nattention_masks = []\n\n\nfor sent in test_sentences:\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      \n                        add_special_tokens = True,\n                        max_length = 80,           \n                        padding='max_length',\n                        truncation=True,\n                        return_attention_mask = True,   \n                        return_tensors = 'pt',     \n                   )\n      \n    input_ids.append(encoded_dict['input_ids'])\n    \n    attention_masks.append(encoded_dict['attention_mask'])\n\ntest_input_ids = torch.cat(input_ids, dim=0)\ntest_attention_masks = torch.cat(attention_masks, dim=0)\ntest_labels = torch.tensor(test_labels)\n\nprint('Original: ', test_sentences[10])\nprint('Token IDs:', test_input_ids[10])\nprint('Attention mask', test_attention_masks[10])","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:30.491217Z","iopub.execute_input":"2021-10-05T17:37:30.491566Z","iopub.status.idle":"2021-10-05T17:37:30.522520Z","shell.execute_reply.started":"2021-10-05T17:37:30.491538Z","shell.execute_reply":"2021-10-05T17:37:30.520943Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":"Il dataset viene diviso randomicamente in 90% training set e 10% validation set","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, random_split\n\n# Si combinano gli input di training in un TensorDataset.\ntrain_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\ntest_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n\n# Calcoliamo il numero di samples da includere in ciascun set.\ntrain_size = int(0.9 * len(train_dataset))\nval_size = len(train_dataset) - train_size\n\n# Dividiamo il dataset randomicamente.\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:30.524466Z","iopub.execute_input":"2021-10-05T17:37:30.525011Z","iopub.status.idle":"2021-10-05T17:37:30.535366Z","shell.execute_reply.started":"2021-10-05T17:37:30.524969Z","shell.execute_reply":"2021-10-05T17:37:30.534208Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":"Dato che il task di nostro interesse consiste nella classificazione di frasi, importiamo la classe **BertForSequenceClassification** per il fine-tuning del modello BERT.","metadata":{}},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, AdamW, BertConfig\n \nmodel = BertForSequenceClassification.from_pretrained(\n    \"dbmdz/bert-base-italian-xxl-cased\", # lo stesso usato per il tokenizzatore\n    num_labels = 2,  # il numero di classi che ci interessa\n    output_attentions = False, \n    output_hidden_states = False,\n)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:30.537119Z","iopub.execute_input":"2021-10-05T17:37:30.538038Z","iopub.status.idle":"2021-10-05T17:37:34.837032Z","shell.execute_reply.started":"2021-10-05T17:37:30.537990Z","shell.execute_reply":"2021-10-05T17:37:34.836052Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.nn import functional as F\n\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\n\nmodel.to(device)\nmodel.train()\n\noptim = AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n\nloaders = {\"train\" : train_loader,\n           \"val\" : val_loader,\n           \"test\" : test_loader}\n\nhistory_loss = {\"train\":[], \"val\":[], \"test\":[]}\nhistory_accuracy = {\"train\":[], \"val\":[], \"test\":[]}\n\ntry:\n  for epoch in range(5):\n    sum_loss = {\"train\":0, \"val\":0, \"test\":0}\n    sum_accuracy = {\"train\":0, \"val\":0, \"test\":0}\n    for split in [\"train\",\"val\",\"test\"]:\n      print(f\"Processing {split}\")\n      for batch in loaders[split]:\n          optim.zero_grad()\n          # `batch` contains three pytorch tensors:\n          #  [0]: input ids \n          #  [1]: attention masks\n          #  [2]: labels\n          input_ids = batch[0].to(device)\n          attn_mask = batch[1].to(device)\n          labels = batch[2].to(device)\n          output = model(input_ids, token_type_ids=None, attention_mask=attn_mask, labels=labels, return_dict=True)\n          loss = output[0]\n          #logits -> batch_size x num_labels\n          logits = output[1]\n          sum_loss[split] += loss.item()\n          if split == \"train\":\n            loss.backward()\n            optim.step()\n          pred = torch.argmax(logits, dim = 1)\n          batch_accuracy = (pred == labels).sum().item()/(input_ids.size(0))\n          sum_accuracy[split] += batch_accuracy\n    epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\",\"val\",\"test\"]}\n    epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\",\"val\",\"test\"]}\n    for split in[\"train\",\"val\",\"test\"]:\n      history_loss[split].append(epoch_loss[split])\n      history_accuracy[split].append(epoch_accuracy[split])\n    print(f\"Epoch {epoch+1}:\",\n          f\"TrL={epoch_loss['train']:.4f},\",\n          f\"TrA={epoch_accuracy['train']:.4f},\",\n          f\"VL={epoch_loss['val']:.4f},\",\n          f\"VA={epoch_accuracy['val']:.4f},\",\n          f\"TeL={epoch_loss['test']:.4f},\",\n          f\"TeA={epoch_accuracy['test']:.4f},\")\nexcept KeyboardInterrupt:\n  print(\"interrupted\")\nfinally:\n  plt.title(\"loss\")\n  for split in [\"train\",\"val\",\"test\"]:\n    plt.plot(history_loss[split], label=split)\n  plt.legend()\n  plt.show()\n\n  plt.title(\"accuracy\")\n  for split in [\"train\",\"val\",\"test\"]:\n    plt.plot(history_accuracy[split], label=split)\n  plt.legend()\n  plt.show()\n    \n#torch.save(model.state_dict(), \"./modello5.4\")","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:34.838965Z","iopub.execute_input":"2021-10-05T17:37:34.839496Z","iopub.status.idle":"2021-10-05T17:37:42.554939Z","shell.execute_reply.started":"2021-10-05T17:37:34.839451Z","shell.execute_reply":"2021-10-05T17:37:42.554214Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:42.558001Z","iopub.execute_input":"2021-10-05T17:37:42.558255Z","iopub.status.idle":"2021-10-05T17:37:42.568688Z","shell.execute_reply.started":"2021-10-05T17:37:42.558229Z","shell.execute_reply":"2021-10-05T17:37:42.567621Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"markdown","source":"Per avere ulteriori metriche di riferimento per la valutazione del modello, abbiamo deciso di plottare la **matrice di confusione** e di calcolare il **classification report** sui risultati della parte di test.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\nx_test = test_sentences\ny_test = test_labels\n\ny_pred=[]\nfor x in x_test:\n    input_encodings = tokenizer(x, return_tensors='pt')\n    input_ids = input_encodings[\"input_ids\"].to(device)\n    attn_mask = input_encodings[\"attention_mask\"].to(device)\n    output = model(input_ids, attention_mask=attn_mask)\n    pred = torch.argmax(output[0], dim = 1)\n    y_pred.append(pred.item())    \n    \nprint(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:42.570329Z","iopub.execute_input":"2021-10-05T17:37:42.570727Z","iopub.status.idle":"2021-10-05T17:37:43.091785Z","shell.execute_reply.started":"2021-10-05T17:37:42.570686Z","shell.execute_reply":"2021-10-05T17:37:43.090881Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"cm= confusion_matrix(y_test,y_pred)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot() ","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:43.093090Z","iopub.execute_input":"2021-10-05T17:37:43.093595Z","iopub.status.idle":"2021-10-05T17:37:43.233942Z","shell.execute_reply.started":"2021-10-05T17:37:43.093555Z","shell.execute_reply":"2021-10-05T17:37:43.232828Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"markdown","source":"Possiamo testare manualmente il nostro modello con frasi di prova.","metadata":{}},{"cell_type":"code","source":"frase = \"voglio che sia freddo come il ghiaccio\"\n\ninput_encodings = tokenizer(frase, return_tensors='pt')\ninput_ids = input_encodings[\"input_ids\"].to(device)\nattn_mask = input_encodings[\"attention_mask\"].to(device)\noutput = model(input_ids, attention_mask=attn_mask)\npred = torch.argmax(output[0], dim = 1)\nprint(pred.item())","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:43.235473Z","iopub.execute_input":"2021-10-05T17:37:43.235846Z","iopub.status.idle":"2021-10-05T17:37:43.262254Z","shell.execute_reply.started":"2021-10-05T17:37:43.235808Z","shell.execute_reply":"2021-10-05T17:37:43.261197Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"frase = \"m'illumino d'immenso\"\n\ninput_encodings = tokenizer(frase, return_tensors='pt')\ninput_ids = input_encodings[\"input_ids\"].to(device)\nattn_mask = input_encodings[\"attention_mask\"].to(device)\noutput = model(input_ids, attention_mask=attn_mask)\npred = torch.argmax(output[0], dim = 1)\nprint(pred.item())","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:37:43.263809Z","iopub.execute_input":"2021-10-05T17:37:43.264222Z","iopub.status.idle":"2021-10-05T17:37:43.290692Z","shell.execute_reply.started":"2021-10-05T17:37:43.264183Z","shell.execute_reply":"2021-10-05T17:37:43.289838Z"},"trusted":true},"execution_count":138,"outputs":[]}]}